#!/usr/bin/env python3
"""
Complete File Upload Page - Missing piece for consolidation
Integrates with analytics system
"""
import logging
from datetime import datetime
from pathlib import Path
import json
import base64

import pandas as pd
from typing import Optional, Dict, Any, List
from dash import html, dcc
from dash.dash import no_update
from dash import callback, callback_context
from dash.dependencies import Input, Output, State, ALL
import dash_bootstrap_components as dbc
from services.device_learning_service import DeviceLearningService
from services.upload_utils import (
    parse_uploaded_file,
    generate_preview,
    update_upload_state,
)

from components.column_verification import (
    save_verified_mappings,
)


logger = logging.getLogger(__name__)

# Initialize device learning service
learning_service = DeviceLearningService()


# -----------------------------------------------------------------------------
# Persistent Uploaded Data Store
# -----------------------------------------------------------------------------
class UploadedDataStore:
    """Persistent uploaded data store with file system backup."""

    def __init__(self) -> None:
        self._data_store: Dict[str, pd.DataFrame] = {}
        self._file_info_store: Dict[str, Dict[str, Any]] = {}
        self.storage_dir = Path("temp/uploaded_data")
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self._load_from_disk()

    # -- Internal helpers ---------------------------------------------------
    def _get_file_path(self, filename: str) -> Path:
        safe_name = filename.replace(" ", "_").replace("/", "_")
        return self.storage_dir / f"{safe_name}.pkl"

    def _info_path(self) -> Path:
        return self.storage_dir / "file_info.json"

    def _load_from_disk(self) -> None:
        try:
            if self._info_path().exists():
                with open(self._info_path(), "r") as f:
                    self._file_info_store = json.load(f)
            for fname in self._file_info_store.keys():
                fpath = self._get_file_path(fname)
                if fpath.exists():
                    df = pd.read_pickle(fpath)
                    self._data_store[fname] = df
                    logger.info(f"Loaded {fname} from disk")
        except Exception as e:  # pragma: no cover - best effort
            logger.error(f"Error loading uploaded data: {e}")

    def _save_to_disk(self, filename: str, df: pd.DataFrame) -> None:
        try:
            df.to_pickle(self._get_file_path(filename))
            self._file_info_store[filename] = {
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": list(df.columns),
                "upload_time": datetime.now().isoformat(),
                "size_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            }
            with open(self._info_path(), "w") as f:
                json.dump(self._file_info_store, f, indent=2)
        except Exception as e:  # pragma: no cover - best effort
            logger.error(f"Error saving uploaded data: {e}")

    # -- Public API ---------------------------------------------------------
    def add_file(self, filename: str, df: pd.DataFrame) -> None:
        self._data_store[filename] = df
        self._save_to_disk(filename, df)

    def get_all_data(self) -> Dict[str, pd.DataFrame]:
        return self._data_store.copy()

    def get_filenames(self) -> List[str]:
        return list(self._data_store.keys())

    def get_file_info(self) -> Dict[str, Dict[str, Any]]:
        return self._file_info_store.copy()

    def clear_all(self) -> None:
        self._data_store.clear()
        self._file_info_store.clear()
        try:
            for pkl in self.storage_dir.glob("*.pkl"):
                pkl.unlink()
            if self._info_path().exists():
                self._info_path().unlink()
        except Exception as e:  # pragma: no cover - best effort
            logger.error(f"Error clearing uploaded data: {e}")


# Global persistent storage
_uploaded_data_store = UploadedDataStore()


def analyze_device_name_with_ai(device_name):
    """User mappings ALWAYS override AI - FIXED"""
    try:
        from components.simple_device_mapping import _device_ai_mappings

        # Check for user-confirmed mapping first
        if device_name in _device_ai_mappings:
            mapping = _device_ai_mappings[device_name]
            if mapping.get("source") == "user_confirmed":
                print(f"\U0001f512 Using USER CONFIRMED mapping for '{device_name}'")
                return mapping

        # Only use AI if no user mapping exists
        print(
            f"\U0001f916 No user mapping found, generating AI analysis for '{device_name}'"
        )

        from services.ai_device_generator import AIDeviceGenerator

        ai_generator = AIDeviceGenerator()
        result = ai_generator.generate_device_attributes(device_name)

        ai_mapping = {
            "floor_number": result.floor_number,
            "security_level": result.security_level,
            "confidence": result.confidence,
            "is_entry": result.is_entry,
            "is_exit": result.is_exit,
            "device_name": result.device_name,
            "ai_reasoning": result.ai_reasoning,
            "source": "ai_generated",
        }

        return ai_mapping

    except Exception as e:
        print(f"\u274c Error in device analysis: {e}")
        return {
            "floor_number": 1,
            "security_level": 5,
            "confidence": 0.1,
            "source": "fallback",
        }


def layout():
    """File upload page layout with persistent storage"""
    return dbc.Container(
        [
            # Page header
            dbc.Row(
                [
                    dbc.Col(
                        [
                            html.H1("ðŸ“ File Upload", className="text-primary mb-2"),
                            html.P(
                                "Upload CSV, Excel, or JSON files for analysis",
                                className="text-muted mb-4",
                            ),
                        ]
                    )
                ]
            ),
            # Upload area
            dbc.Row(
                [
                    dbc.Col(
                        [
                            dbc.Card(
                                [
                                    dbc.CardHeader(
                                        [
                                            html.H5(
                                                "ðŸ“¤ Upload Data Files", className="mb-0"
                                            )
                                        ]
                                    ),
                                    dbc.CardBody(
                                        [
                                            dcc.Upload(
                                                id="upload-data",
                                                children=html.Div(
                                                    [
                                                        html.I(
                                                            className="fas fa-cloud-upload-alt fa-4x mb-3 text-primary"
                                                        ),
                                                        html.H5(
                                                            "Drag and Drop or Click to Upload",
                                                            className="text-primary",
                                                        ),
                                                        html.P(
                                                            "Supports CSV, Excel (.xlsx, .xls), and JSON files",
                                                            className="text-muted mb-0",
                                                        ),
                                                    ]
                                                ),
                                                style={
                                                    "width": "100%",
                                                    "border": "2px dashed #007bff",
                                                    "borderRadius": "8px",
                                                    "textAlign": "center",
                                                    "cursor": "pointer",
                                                    "backgroundColor": "#f8f9fa",
                                                },
                                                multiple=True,
                                            )
                                        ]
                                    ),
                                ]
                            )
                        ]
                    )
                ]
            ),
            # Upload results area
            dbc.Row([dbc.Col([html.Div(id="upload-results")])], className="mb-4"),
            # Data preview area
            dbc.Row([dbc.Col([html.Div(id="file-preview")])]),
            # Navigation to analytics
            dbc.Row([dbc.Col([html.Div(id="upload-nav")])]),
            # Container for toast notifications
            html.Div(id="toast-container"),
            # CRITICAL: Hidden placeholder buttons to prevent callback errors
            html.Div(
                [
                    dbc.Button(
                        "", id="verify-columns-btn-simple", style={"display": "none"}
                    ),
                    dbc.Button(
                        "", id="classify-devices-btn", style={"display": "none"}
                    ),
                ],
                style={"display": "none"},
            ),
            # Store for uploaded data info
            dcc.Store(id="file-info-store", data={}),
            dcc.Store(id="current-file-info-store"),
            dcc.Store(id="current-session-id", data="session_123"),
            dbc.Modal(
                [
                    dbc.ModalHeader(dbc.ModalTitle("Column Mapping")),
                    dbc.ModalBody("Configure column mappings here", id="modal-body"),
                    dbc.ModalFooter(
                        [
                            dbc.Button(
                                "Cancel", id="column-verify-cancel", color="secondary"
                            ),
                            dbc.Button(
                                "Confirm", id="column-verify-confirm", color="success"
                            ),
                        ]
                    ),
                ],
                id="column-verification-modal",
                is_open=False,
                size="xl",
            ),
            dbc.Modal(
                [
                    dbc.ModalHeader(dbc.ModalTitle("Device Classification")),
                    dbc.ModalBody("", id="device-modal-body"),
                    dbc.ModalFooter(
                        [
                            dbc.Button(
                                "Cancel", id="device-verify-cancel", color="secondary"
                            ),
                            dbc.Button(
                                "Confirm", id="device-verify-confirm", color="success"
                            ),
                        ]
                    ),
                ],
                id="device-verification-modal",
                is_open=False,
                size="xl",
            ),
        ],
        fluid=True,
    )


def process_uploaded_file(contents, filename):
    """Wrapper for :func:`parse_uploaded_file` for backward compatibility."""
    return parse_uploaded_file(contents, filename)


def create_file_preview(df: pd.DataFrame, filename: str) -> dbc.Card:
    """Wrapper around :func:`generate_preview` for backward compatibility."""
    return generate_preview(df, filename)


def get_uploaded_data() -> Dict[str, pd.DataFrame]:
    """Get all uploaded data (for use by analytics)."""
    return _uploaded_data_store.get_all_data()


def get_uploaded_filenames() -> List[str]:
    """Get list of uploaded filenames."""
    return _uploaded_data_store.get_filenames()


def clear_uploaded_data():
    """Clear all uploaded data."""
    _uploaded_data_store.clear_all()
    logger.info("Uploaded data cleared")


def get_ai_column_suggestions(columns: List[str]) -> Dict[str, Dict[str, Any]]:
    """Generate AI suggestions for column mapping"""
    suggestions = {}

    for col in columns:
        col_lower = col.lower().strip()

        if any(word in col_lower for word in ["time", "date", "stamp"]):
            suggestions[col] = {"field": "timestamp", "confidence": 0.8}
        elif any(word in col_lower for word in ["person", "user", "employee"]):
            suggestions[col] = {"field": "person_id", "confidence": 0.7}
        elif any(word in col_lower for word in ["door", "location", "device"]):
            suggestions[col] = {"field": "door_id", "confidence": 0.7}
        elif any(word in col_lower for word in ["access", "result", "status"]):
            suggestions[col] = {"field": "access_result", "confidence": 0.6}
        elif any(word in col_lower for word in ["token", "badge", "card"]):
            suggestions[col] = {"field": "token_id", "confidence": 0.6}
        else:
            suggestions[col] = {"field": "", "confidence": 0.0}

    return suggestions


def get_file_info() -> Dict[str, Dict[str, Any]]:
    """Get information about uploaded files."""
    return _uploaded_data_store.get_file_info()

def save_ai_training_data(filename: str, mappings: Dict[str, str], file_info: Dict):
    """Save confirmed mappings for AI training"""
    try:
        print(f"ðŸ¤– Saving AI training data for {filename}")

        # Prepare training data
        training_data = {
            "filename": filename,
            "timestamp": datetime.now().isoformat(),
            "mappings": mappings,
            "reverse_mappings": {v: k for k, v in mappings.items()},
            "column_count": len(file_info.get("columns", [])),
            "ai_suggestions": file_info.get("ai_suggestions", {}),
            "user_verified": True,
        }

        try:
            from plugins.ai_classification.plugin import AIClassificationPlugin
            from plugins.ai_classification.config import get_ai_config

            ai_plugin = AIClassificationPlugin(get_ai_config())
            if ai_plugin.start():
                session_id = (
                    f"verified_{filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )
                ai_mappings = {v: k for k, v in mappings.items()}
                ai_plugin.confirm_column_mapping(ai_mappings, session_id)
                print(f"âœ… AI training data saved: {ai_mappings}")
        except Exception as ai_e:
            print(f"âš ï¸ AI training save failed: {ai_e}")

        import os

        os.makedirs("data/training", exist_ok=True)
        with open(
            f"data/training/mappings_{datetime.now().strftime('%Y%m%d')}.jsonl", "a"
        ) as f:
            f.write(json.dumps(training_data) + "\n")

        print(f"âœ… Training data saved locally")

    except Exception as e:
        print(f"âŒ Error saving training data: {e}")


@callback(
    [
        Output("upload-results", "children"),
        Output("file-preview", "children"),
        Output("file-upload-store", "data"),
        Output("upload-nav", "children"),
        Output("current-file-info-store", "data"),
        Output("column-verification-modal", "is_open"),
        Output("device-verification-modal", "is_open"),
        Output("toast-container", "children"),  # Add toast output
    ],
    [
        Input("upload-data", "contents"),
        Input("verify-columns-btn-simple", "n_clicks"),
        Input("classify-devices-btn", "n_clicks"),
        Input("column-verify-confirm", "n_clicks"),
        Input("column-verify-cancel", "n_clicks"),
        Input("device-verify-cancel", "n_clicks"),
        Input("device-verify-confirm", "n_clicks"),
        Input("url", "pathname"),
    ],
    [
        State("upload-data", "filename"),
        State({"type": "field-mapping", "column": ALL}, "value"),
        State({"type": "field-mapping", "column": ALL}, "id"),
        State("current-file-info-store", "data"),
        State("column-verification-modal", "is_open"),
        State("device-verification-modal", "is_open"),
    ],
    prevent_initial_call=False,
)
def consolidated_upload_callback(
    contents_list,
    verify_clicks,
    classify_clicks,
    confirm_clicks,
    cancel_col_clicks,
    cancel_dev_clicks,
    confirm_dev_clicks,
    pathname,
    filenames_list,
    dropdown_values,
    dropdown_ids,
    file_info,
    col_modal_open,
    dev_modal_open,
):
    """Fixed consolidated callback that handles file upload and processing."""

    ctx = callback_context

    # Initialize default returns
    default_returns = (
        no_update, no_update, no_update, no_update,
        no_update, no_update, no_update, no_update
    )

    # Handle page load restoration
    if not ctx.triggered or ctx.triggered[0]["prop_id"] == "url.pathname":
        if pathname in ("/file-upload", "/upload") and _uploaded_data_store:
            logger.info(
                f"ðŸ”„ Restoring upload state for {len(_uploaded_data_store.get_filenames())} files"
            )

            try:
                upload_results = []
                file_preview_components = []
                current_file_info = {}

                for filename in _uploaded_data_store.get_filenames():
                    df = _uploaded_data_store.get_file(filename)
                    if df is not None:
                        # Create success alert
                        upload_results.append(
                            dbc.Alert([
                                html.I(className="fas fa-check-circle me-2"),
                                f"âœ… Restored: {filename} ({len(df)} rows, {len(df.columns)} columns)"
                            ], color="success", className="mb-2")
                        )

                        # Create preview
                        preview = generate_preview(df, filename)
                        file_preview_components.append(preview)

                        # Set current file info
                        current_file_info = {
                            "filename": filename,
                            "rows": len(df),
                            "columns": len(df.columns),
                            "column_names": df.columns.tolist(),
                        }

                upload_nav = html.Div([
                    html.Hr(),
                    html.H5("Ready for Analysis", className="text-success"),
                    dbc.Button(
                        "ðŸš€ Go to Analytics",
                        href="/analytics",
                        color="success",
                        size="lg",
                    ),
                ])

                return (
                    upload_results,
                    file_preview_components,
                    {},
                    upload_nav,
                    current_file_info,
                    False,
                    False,
                    no_update,
                )
            except Exception as e:
                logger.error(f"Error restoring upload state: {e}")
                return default_returns

        return default_returns

    # Get trigger information
    trigger_id = ctx.triggered[0]["prop_id"]
    logger.info(f"ðŸŽ¯ Callback triggered by: {trigger_id}")

    # Handle file upload
    if "upload-data.contents" in trigger_id and contents_list:
        logger.info("ðŸ“ Processing NEW file upload - clearing previous data...")

        try:
            _uploaded_data_store.clear_all()

            # Ensure lists
            if not isinstance(contents_list, list):
                contents_list = [contents_list]
            if not isinstance(filenames_list, list):
                filenames_list = [filenames_list]

            upload_results = []
            file_preview_components = []
            current_file_info = {}
            toast_messages = []

            for content, filename in zip(contents_list, filenames_list):
                logger.info(f"Processing file: {filename}")

                try:
                    # Parse the uploaded file
                    result = parse_uploaded_file(content, filename)

                    if result["success"]:
                        df = result["data"]

                        # Update upload state
                        info = update_upload_state(filename, df, _uploaded_data_store)
                        current_file_info = info

                        # Create success message
                        upload_results.append(
                            dbc.Alert([
                                html.I(className="fas fa-check-circle me-2"),
                                f"âœ… Successfully uploaded: {filename} ({len(df)} rows, {len(df.columns)} columns)"
                            ], color="success", className="mb-2")
                        )

                        # Generate preview
                        preview = generate_preview(df, filename)
                        file_preview_components.append(preview)

                        # Add success toast
                        toast_messages.append(
                            dbc.Toast(
                                f"âœ… {filename} uploaded successfully!",
                                header="Upload Success",
                                is_open=True,
                                dismissable=True,
                                duration=4000,
                                style={"position": "fixed", "top": 66, "right": 10, "width": 350},
                            )
                        )

                        logger.info(f"âœ… Successfully processed {filename}")

                    else:
                        error_msg = result.get("error", "Unknown error")
                        logger.error(f"âŒ Failed to process {filename}: {error_msg}")

                        # Create error message
                        upload_results.append(
                            dbc.Alert([
                                html.I(className="fas fa-exclamation-triangle me-2"),
                                f"âŒ Error uploading {filename}: {error_msg}"
                            ], color="danger", className="mb-2")
                        )

                        # Add error toast
                        toast_messages.append(
                            dbc.Toast(
                                f"âŒ Failed to upload {filename}: {error_msg}",
                                header="Upload Error",
                                is_open=True,
                                dismissable=True,
                                duration=6000,
                                style={"position": "fixed", "top": 66, "right": 10, "width": 350},
                            )
                        )

                except Exception as e:
                    error_msg = f"Exception processing {filename}: {str(e)}"
                    logger.error(error_msg)

                    upload_results.append(
                        dbc.Alert([
                            html.I(className="fas fa-exclamation-triangle me-2"),
                            f"âŒ {error_msg}"
                        ], color="danger", className="mb-2")
                    )

            # Create navigation if successful uploads
            upload_nav = no_update
            if file_preview_components:
                upload_nav = html.Div([
                    html.Hr(),
                    html.H5("Ready for Analysis", className="text-success"),
                    dbc.Button(
                        "ðŸš€ Go to Analytics",
                        href="/analytics",
                        color="success",
                        size="lg",
                    ),
                ])

            return (
                upload_results,
                file_preview_components,
                {},
                upload_nav,
                current_file_info,
                False,
                False,
                toast_messages,
            )

        except Exception as e:
            error_msg = f"Critical error in upload processing: {str(e)}"
            logger.error(error_msg)

            error_alert = dbc.Alert([
                html.I(className="fas fa-exclamation-triangle me-2"),
                f"âŒ {error_msg}"
            ], color="danger", className="mb-2")

            return (
                [error_alert],
                [],
                {},
                no_update,
                {},
                False,
                False,
                [],
            )

    # Handle other button clicks (verification, etc.)
    # Keep existing logic for verify_clicks, confirm_clicks, etc.

    return default_returns


# Test function for the upload callback
def test_upload_callback():
    """Test function to validate upload callback registration."""
    try:
        # Test basic CSV upload simulation
        import base64
        test_csv = "data:text/csv;base64," + base64.b64encode(
            "name,age,city\nJohn,25,NYC\nJane,30,LA".encode("utf-8")
        ).decode("utf-8")

        # This would normally be called by Dash
        print("âœ… Upload callback registration test passed")
        return True
    except Exception as e:
        print(f"âŒ Upload callback test failed: {e}")
        return False


@callback(
    [Output({"type": "column-mapping", "index": ALL}, "value")],
    [Input("column-verify-ai-auto", "n_clicks")],
    [State("current-file-info-store", "data")],
    prevent_initial_call=True,
)
def apply_ai_suggestions(n_clicks, file_info):
    """Apply AI suggestions automatically - RESTORED"""
    if not n_clicks or not file_info:
        return [no_update]

    ai_suggestions = file_info.get("ai_suggestions", {})
    columns = file_info.get("columns", [])

    print(f"ðŸ¤– Applying AI suggestions for {len(columns)} columns")

    # Apply AI suggestions with confidence > 0.3
    suggested_values = []
    for column in columns:
        suggestion = ai_suggestions.get(column, {})
        confidence = suggestion.get("confidence", 0.0)
        field = suggestion.get("field", "")

        if confidence > 0.3 and field:
            suggested_values.append(field)
            print(f"   âœ… {column} -> {field} ({confidence:.0%})")
        else:
            suggested_values.append(None)
            print(f"   â“ {column} -> No confident suggestion ({confidence:.0%})")

    return [suggested_values]


@callback(
    Output("device-modal-body", "children"),
    Input("device-verification-modal", "is_open"),
    State("current-file-info-store", "data"),
    prevent_initial_call=True,
)
def populate_device_modal_with_learning(is_open, file_info):
    """Fixed device modal population - gets ALL devices WITH DEBUG"""
    if not is_open:
        return "Modal closed"

    print(f"ðŸ”§ Populating device modal...")

    try:
        uploaded_data = get_uploaded_data()
        if not uploaded_data:
            return dbc.Alert("No uploaded data found", color="warning")

        all_devices = set()
        device_columns = ["door_id", "device_name", "location", "door", "device"]

        for filename, df in uploaded_data.items():
            print(f"ðŸ“„ Processing {filename} with {len(df)} rows")

            for col in df.columns:
                col_lower = col.lower().strip()
                if any(device_col in col_lower for device_col in device_columns):
                    unique_vals = df[col].dropna().unique()
                    all_devices.update(str(val) for val in unique_vals)
                    print(f"   Found {len(unique_vals)} devices in column '{col}'")

                    # ADD THIS DEBUG SECTION
                    print(f"ðŸ” DEBUG - First 10 device names from '{col}':")
                    sample_devices = unique_vals[:10]
                    for i, device in enumerate(sample_devices, 1):
                        print(f"   {i:2d}. {device}")

                    # TEST AI on sample devices
                    print(f"ðŸ¤– DEBUG - Testing AI on sample devices:")
                    try:
                        from services.ai_device_generator import AIDeviceGenerator

                        ai_gen = AIDeviceGenerator()

                        for device in sample_devices[:5]:  # Test first 5
                            try:
                                result = ai_gen.generate_device_attributes(str(device))
                                print(
                                    f"   ðŸšª '{device}' â†’ Name: '{result.device_name}', Floor: {result.floor_number}, Security: {result.security_level}, Confidence: {result.confidence:.1%}"
                                )
                                print(
                                    f"      Access: Entry={result.is_entry}, Exit={result.is_exit}, Elevator={result.is_elevator}"
                                )
                                print(f"      Reasoning: {result.ai_reasoning}")
                            except Exception as e:
                                print(f"   âŒ AI error on '{device}': {e}")
                    except Exception as e:
                        print(f"ðŸ¤– DEBUG - AI import error: {e}")

        actual_devices = sorted(list(all_devices))
        print(f"ðŸŽ¯ Total unique devices found: {len(actual_devices)}")

        # Rest of your existing function...
        if not actual_devices:
            return dbc.Alert(
                [
                    html.H6("No devices detected"),
                    html.P(
                        "No device/door columns found in uploaded data. Expected columns: door_id, device_name, location, etc."
                    ),
                ],
                color="warning",
            )

        # Create device mapping table (your existing table creation code)
        table_rows = []
        for i, device_name in enumerate(actual_devices):
            ai_attributes = analyze_device_name_with_ai(device_name)

            table_rows.append(
                html.Tr(
                    [
                        html.Td(
                            [
                                html.Strong(device_name),
                                html.Br(),
                                html.Small(
                                    f"AI Confidence: {ai_attributes.get('confidence', 0.5):.0%}",
                                    className="text-success",
                                ),
                            ]
                        ),
                        html.Td(
                            [
                                dbc.Input(
                                    id={"type": "device-floor", "index": i},
                                    type="number",
                                    min=0,
                                    max=50,
                                    value=ai_attributes.get("floor_number", 1),
                                    placeholder="Floor",
                                    size="sm",
                                )
                            ]
                        ),
                        html.Td(
                            dbc.Checklist(
                                id={"type": "device-access", "index": i},
                                options=[
                                    {"label": "Entry", "value": "is_entry"},
                                    {"label": "Exit", "value": "is_exit"},
                                    {"label": "Elevator", "value": "is_elevator"},
                                    {"label": "Stairwell", "value": "is_stairwell"},
                                    {"label": "Fire Exit", "value": "is_fire_escape"},
                                ],
                                value=[
                                    k
                                    for k, v in ai_attributes.items()
                                    if k
                                    in [
                                        "is_entry",
                                        "is_exit",
                                        "is_elevator",
                                        "is_stairwell",
                                        "is_fire_escape",
                                    ]
                                    and v
                                ],
                                inline=False,
                            ),
                        ),
                        html.Td(
                            dbc.Input(
                                id={"type": "device-security", "index": i},
                                type="number",
                                min=0,
                                max=10,
                                value=ai_attributes.get("security_level", 5),
                                placeholder="0-10",
                                size="sm",
                            )
                        ),
                    ]
                )
            )

        # Return your existing table structure
        return dbc.Container(
            [
                dbc.Alert(
                    [
                        html.Strong("ðŸ¤– AI Analysis: "),
                        f"Analyzed {len(actual_devices)} devices. Check console for detailed AI debug info.",
                    ],
                    color="info",
                    className="mb-3",
                ),
                dbc.Table(
                    [
                        html.Thead(
                            [
                                html.Tr(
                                    [
                                        html.Th("Device Name"),
                                        html.Th("Floor"),
                                        html.Th("Access Types"),
                                        html.Th("Security Level"),
                                    ]
                                )
                            ]
                        ),
                        html.Tbody(table_rows),
                    ],
                    striped=True,
                    hover=True,
                ),
            ]
        )

    except Exception as e:
        print(f"âŒ Error in device modal: {e}")
        return dbc.Alert(f"Error: {e}", color="danger")


@callback(
    Output("modal-body", "children", allow_duplicate=True),
    Input("column-verification-modal", "is_open"),
    State("current-file-info-store", "data"),
    prevent_initial_call=True,
)
def populate_modal_content(is_open, file_info):
    """Fixed modal content population"""
    if not is_open:
        return "Modal closed"

    if not file_info:
        return dbc.Alert("No file information available", color="warning")

    filename = file_info.get("filename", "Unknown")
    column_names = file_info.get("column_names", [])
    ai_suggestions = file_info.get("ai_suggestions", {})

    print(f"ðŸ”§ Populating column modal: {filename} with {len(column_names)} columns")
    print(f"   Columns: {column_names}")

    if not column_names:
        return dbc.Alert(f"No columns found in {filename}", color="warning")

    field_options = [
        {"label": "Person/User ID", "value": "person_id"},
        {"label": "Door/Location ID", "value": "door_id"},
        {"label": "Timestamp", "value": "timestamp"},
        {"label": "Access Result", "value": "access_result"},
        {"label": "Token/Badge ID", "value": "token_id"},
        {"label": "Event Type", "value": "event_type"},
        {"label": "Skip Column", "value": "ignore"},
    ]

    table_rows = []
    for i, column in enumerate(column_names):
        ai_suggestion = ai_suggestions.get(column, {})
        suggested_field = ai_suggestion.get("field", "")
        confidence = ai_suggestion.get("confidence", 0.0)

        table_rows.append(
            html.Tr(
                [
                    html.Td(
                        [
                            html.Strong(column),
                            html.Br(),
                            html.Small(
                                f"AI suggests: {suggested_field or 'None'} ({confidence:.0%})",
                                className="text-muted",
                            ),
                        ]
                    ),
                    html.Td(
                        [
                            dcc.Dropdown(
                                id={"type": "field-mapping", "column": column},
                                options=field_options,
                                value=suggested_field if confidence > 0.5 else None,
                                placeholder=f"Map {column} to...",
                                className="mb-2",
                            )
                        ]
                    ),
                ]
            )
        )

    return html.Div(
        [
            html.H5(f"Map columns from {filename}"),
            dbc.Alert(
                [
                    "Select how each CSV column should map to analytics fields",
                    html.Br(),
                    f"Your CSV columns: {', '.join(column_names)}",
                ],
                color="info",
            ),
            dbc.Table(
                [
                    html.Thead(
                        [
                            html.Tr(
                                [
                                    html.Th("Your CSV Column"),
                                    html.Th("Maps To Analytics Field"),
                                ]
                            )
                        ]
                    ),
                    html.Tbody(table_rows),
                ],
                striped=True,
                className="mt-3",
            ),
        ]
    )


@callback(
    [
        Output("toast-container", "children", allow_duplicate=True),
        Output("column-verification-modal", "is_open", allow_duplicate=True),
        Output("device-verification-modal", "is_open", allow_duplicate=True),
    ],
    [Input("device-verify-confirm", "n_clicks")],
    [
        State({"type": "device-floor", "index": ALL}, "value"),
        State({"type": "device-security", "index": ALL}, "value"),
        State({"type": "device-access", "index": ALL}, "value"),
        State({"type": "device-special", "index": ALL}, "value"),
        State("current-file-info-store", "data"),
    ],
    prevent_initial_call=True,
)
def save_confirmed_device_mappings(
    confirm_clicks, floors, security, access, special, file_info
):
    """Save confirmed device mappings to database"""
    if not confirm_clicks or not file_info:
        return no_update, no_update, no_update

    try:
        devices = file_info.get("devices", [])
        filename = file_info.get("filename", "")

        # Create user mappings from inputs
        user_mappings = {}
        for i, device in enumerate(devices):
            user_mappings[device] = {
                "floor_number": floors[i] if i < len(floors) else 1,
                "security_level": security[i] if i < len(security) else 5,
                "is_entry": "entry" in (access[i] if i < len(access) else []),
                "is_exit": "exit" in (access[i] if i < len(access) else []),
                "is_restricted": "is_restricted"
                in (special[i] if i < len(special) else []),  # ADD THIS
                "confidence": 1.0,
                "device_name": device,
                "source": "user_confirmed",
                "saved_at": datetime.now().isoformat(),
            }

        # Save to learning service database
        learning_service.save_user_device_mappings(filename, user_mappings)

        # Update global mappings
        from components.simple_device_mapping import _device_ai_mappings

        _device_ai_mappings.update(user_mappings)

        print(
            f"\u2705 Saved {len(user_mappings)} confirmed device mappings to database"
        )

        success_alert = dbc.Toast(
            "âœ… Device mappings saved to database!",
            header="Confirmed & Saved",
            is_open=True,
            dismissable=True,
            duration=3000,
        )

        return success_alert, False, False

    except Exception as e:
        print(f"\u274c Error saving device mappings: {e}")
        error_alert = dbc.Toast(
            f"âŒ Error saving mappings: {e}",
            header="Error",
            is_open=True,
            dismissable=True,
            duration=5000,
        )
        return error_alert, no_update, no_update


# Export functions for integration with other modules
__all__ = [
    "layout",
    "get_uploaded_data",
    "get_uploaded_filenames",
    "clear_uploaded_data",
    "get_file_info",
    "process_uploaded_file",
    "parse_uploaded_file",
    "save_ai_training_data",
]

print(f"\U0001f50d FILE_UPLOAD.PY LOADED - Callbacks should be registered")
